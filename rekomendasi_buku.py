# -*- coding: utf-8 -*-
"""Rekomendasi Buku.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TAEfW0Fpsvp4Y0LwB_BTQyNXD11WymYh

# Import Libraries
"""

# Import library utama untuk analisis data dan visualisasi
import pandas as pd
import numpy as np # Make sure this line is executed
import matplotlib.pyplot as plt
import seaborn as sns

# Import library untuk Content-based Filtering
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Import library TensorFlow untuk deep learning
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, regularizers

# Import untuk preprocessing
from sklearn.preprocessing import LabelEncoder

# Import Path untuk memudahkan pengelolaan path file
from pathlib import Path

# Import untuk menampilkan tabel dengan format rapi
from tabulate import tabulate

"""# Data Understanding"""

# Upload file CSV
from google.colab import files
uploaded = files.upload()

df_Ratings = pd.read_csv('Ratings.csv')
df_Users = pd.read_csv('Users.csv')
df_Books = pd.read_csv('Books.csv')

"""## Rating"""

df_Ratings.info()

df_Ratings.head()

df_Ratings.groupby('Book-Rating').count()

df_Ratings['User-ID'].value_counts()

df_Ratings['ISBN'].value_counts()

rating_counter = df_Ratings.groupby('Book-Rating').count()
plt.figure(figsize=(10,5))
plt.title('Jumlah Rating Buku yang Diberikan Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah Buku')
plt.bar(rating_counter.index, rating_counter['ISBN'])
plt.grid(True)
plt.show()

print(df_Ratings.isnull().sum())

"""## User"""

df_Users.info()

df_Users.head()

print(df_Users.isnull().sum())

# Gabungkan data yang valid
valid_ages = df_Users[(df_Users['Age'] >= 5) & (df_Users['Age'] <= 100)]

# Hitung rata-rata umur per lokasi, lalu ambil 10 besar
avg_age_by_location = valid_ages.groupby('Location')['Age'].mean().sort_values(ascending=False).head(10)

plt.figure(figsize=(12, 6))
sns.barplot(x=avg_age_by_location.values, y=avg_age_by_location.index, palette='coolwarm')
plt.title('Rata-rata Usia Pengguna di 10 Lokasi Teratas')
plt.xlabel('Rata-rata Usia')
plt.ylabel('Lokasi')
plt.show()

"""## Books"""

df_Books.info()

df_Books.head()

print(df_Books.isnull().sum())

print('Banyak buku: ', len(df_Books.ISBN.unique()))

df_Books['Book-Author'].value_counts()

df_Books['Publisher'].value_counts()

"""# Data Preprocessing"""

# Ambil ISBN unik dari kedua DataFrame lalu gabungkan dan urutkan
isbn_buku = df_Books['ISBN'].drop_duplicates()
isbn_rating = df_Ratings['ISBN'].drop_duplicates()
semua_isbn = np.unique(np.concatenate([isbn_buku, isbn_rating]))

# Tampilkan jumlah ISBN unik
print(f'Total ISBN unik dari semua data: {len(semua_isbn)}')

"""Proses ini bertujuan menggabungkan seluruh ISBN yang tersedia, baik dari data buku maupun data rating,
agar diperoleh daftar lengkap ISBN unik yang menjadi dasar sistem rekomendasi atau analisis selanjutnya.

"""

# Ekstraksi User-ID dari df_Ratings dan df_Users lalu gabungkan
user_dari_rating = df_Ratings['User-ID'].drop_duplicates()
user_dari_users = df_Users['User-ID'].drop_duplicates()
seluruh_user = np.unique(np.concatenate([user_dari_rating, user_dari_users]))

# Tampilkan jumlah total user unik
print(f'Total user unik: {len(seluruh_user)}')

""" Menggabungkan seluruh user dari data rating dan data pengguna penting untuk memastikan.

"""

# Lakukan merge antara df_Books dan df_Ratings berdasarkan ISBN
data_buku_rating = df_Books.merge(df_Ratings, on='ISBN', how='inner')

# Lihat hasil penggabungan awal
data_buku_rating.head()

# Gabungkan data buku-rating dengan df_Users berdasarkan User-ID
data_lengkap = data_buku_rating.merge(df_Users, on='User-ID', how='inner')

# Tampilkan beberapa baris data hasil akhir
data_lengkap.head()

# Cek jumlah nilai kosong pada setiap kolom
data_lengkap.isna().sum()

"""# Data Preparation

Membersihkan missing value atau Menghapus data yang memiliki nilai kosong
"""

# Menghapus baris yang memiliki nilai NaN dari DataFrame 'data_lengkap'
clean_books = data_lengkap.dropna()
clean_books

"""Memverifikasi tidak ada nilai kosong"""

# Menampilkan jumlah missing value pada setiap kolom setelah pembersihan
clean_books.isna().sum()

# Menyusun ulang data berdasarkan kolom ISBN dari A ke Z
sorted_books = clean_books.sort_values(by='ISBN', ascending=True)
sorted_books

"""Menyalin ke variabel baru dan mengurutkan ulang"""

# Menyimpan ulang ke variabel baru dan mengurutkan lagi berdasarkan ISBN
prepared_books = sorted_books.copy()
prepared_books = prepared_books.sort_values(by='ISBN')
prepared_books

# Menghapus entri duplikat berdasarkan ISBN
prepared_books = prepared_books.drop_duplicates(subset='ISBN')
prepared_books

"""Proses ini menyiapkan data dengan mengambil sampel acak dan menyaring hanya buku yang memiliki rating valid (> 0), guna memastikan efisiensi dan kualitas data untuk pemodelan rekomendasi.

"""

# Ambil sampel acak dari data
sample_books = prepared_books.sample(n=10000, random_state=42)

# Filter data: hanya yang memiliki rating lebih dari 0
books_rating_clean = sample_books[sample_books['Book-Rating'] > 0]

# Lanjutkan proses, misalnya tampilkan beberapa data
# Display the DataFrame
display(books_rating_clean)

"""Mengonversi kolom ISBN,Book title, dan Book author ke dalam bentuk list"""

# Mengubah tiga kolom menjadi list untuk digunakan dalam pemodelan
list_ids = sample_books['ISBN'].to_list()
list_titles = sample_books['Book-Title'].to_list()
list_authors = sample_books['Book-Author'].to_list()

print(f"Jumlah ID: {len(list_ids)}")
print(f"Jumlah Judul: {len(list_titles)}")
print(f"Jumlah Penulis: {len(list_authors)}")

# Menyusun DataFrame baru dengan struktur yang dibutuhkan
books_final = pd.DataFrame({
    'id': list_ids,
    'book_title': list_titles,
    'book_author': list_authors
})
books_final

"""# Model Development dengan Content Based Filtering"""

# Menyalin dataset akhir ke variabel baru
dataset = books_final.copy()

# Menampilkan 5 baris acak dari dataset
dataset.sample(n=5)

"""Melakukan Inisialisasi dan pelatihan TF-IDF Vectorizer"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Membuat instance dari TfidfVectorizer
vectorizer = TfidfVectorizer()

# Melatih vectorizer menggunakan data penulis buku
vectorizer.fit(dataset['book_author'])

# Menampilkan semua fitur (kata unik)
vectorizer.get_feature_names_out()

"""melakukan transformasi teks ke dalam bentuk matriks TF-IDF"""

# Mengubah data penulis menjadi representasi vektor TF-IDF
tfidf_result = vectorizer.fit_transform(dataset['book_author'])

# Menampilkan dimensi dari hasil transformasi
tfidf_result.shape

# Mengubah sparse matrix menjadi bentuk dense (matriks penuh)
tfidf_dense = tfidf_result.todense()
tfidf_dense

"""Menampilkan matriks TF-IDF dalam bentuk tabel"""

# Mengonversi matrix ke DataFrame untuk visualisasi
tfidf_df = pd.DataFrame(
    tfidf_dense,
    columns=vectorizer.get_feature_names_out(),
    index=dataset['book_title']
)

# Menampilkan 10 baris dan 22 kolom secara acak
tfidf_df.sample(n=10).sample(n=22, axis=1)

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity antara semua buku
similarity_scores = cosine_similarity(tfidf_result)
similarity_scores

"""Membuat DataFrame dari cosine similarity"""

# Membentuk dataframe similarity dengan label judul buku
similarity_df = pd.DataFrame(similarity_scores, index=dataset['book_title'], columns=dataset['book_title'])

# Menampilkan ukuran matriks
print("Dimensi matriks:", similarity_df.shape)

# Menampilkan sebagian data
similarity_df.sample(n=10).sample(n=5, axis=1)

"""Fungsi rekomendasi buku"""

def rekomendasi_buku(judul, similarity_data=similarity_df, metadata=dataset[['book_title', 'book_author']], jumlah=5):
    """
    Menghasilkan daftar buku yang mirip berdasarkan judul input.
    """
    # Menemukan indeks kemiripan tertinggi (k+1 karena termasuk dirinya sendiri)
    indeks_terdekat = similarity_data.loc[:, judul].to_numpy().argpartition(range(-1, -jumlah, -1))

    # Ambil nama-nama judul teratas
    rekomendasi = similarity_data.columns[indeks_terdekat[-1:-(jumlah+2):-1]]

    # Menghapus buku input dari hasil rekomendasi
    rekomendasi = rekomendasi.drop(judul, errors='ignore')

    return pd.DataFrame(rekomendasi).merge(metadata).head(jumlah)

"""Menampilkan info buku berdasarkan judul"""

# Menampilkan data buku berdasarkan judul tertentu
dataset[dataset['book_title'] == 'Letter to Lord Liszt']

"""Jalankan fungsi rekomendasi"""

# Menampilkan rekomendasi untuk judul yang diberikan
rekomendasi_buku('Letter to Lord Liszt')

# Menetapkan nilai ambang batas untuk kemiripan
nilai_ambang = 0.5

# Membuat matriks ground truth biner
truth_matrix = (similarity_scores >= nilai_ambang).astype(int)

# Mengambil sebagian data untuk visualisasi
pd.DataFrame(truth_matrix, index=dataset['book_title'], columns=dataset['book_title']).sample(10).sample(5, axis=1)

# Menentukan jumlah sampel evaluasi
ukuran_sampel = 10000

# Membuat subset dari similarity dan ground truth
sim_sampel = similarity_scores[:ukuran_sampel, :ukuran_sampel]
truth_sampel = truth_matrix[:ukuran_sampel, :ukuran_sampel]

# Ubah ke bentuk satu dimensi
sim_flat = sim_sampel.flatten()
truth_flat = truth_sampel.flatten()

from sklearn.metrics import precision_recall_fscore_support

# Buat prediksi dari cosine similarity dengan threshold
prediksi = (sim_flat >= nilai_ambang).astype(int)

# Evaluasi prediksi
precision, recall, f1, _ = precision_recall_fscore_support(
    truth_flat, prediksi, average='binary', zero_division=1
)

# Menampilkan hasil evaluasi
print(f"Precision : {precision:.2f}")
print(f"Recall    : {recall:.2f}")
print(f"F1-score  : {f1:.2f}")

"""# **Model Collaborative Filtering**

Filter Kolom yang Dibutuhkan
"""

# Memilih kolom yang dibutuhkan dari dataset
selected_cols = ['User-ID', 'ISBN', 'Book-Rating']
ratings_df = books_rating_clean[selected_cols]
ratings_df.head()

"""Proses Encoding User-ID"""

# Mendapatkan daftar user unik
unique_users = ratings_df['User-ID'].drop_duplicates().tolist()

# Mapping dari User-ID ke indeks numerik
user_id_to_index = {user_id: idx for idx, user_id in enumerate(unique_users)}

# Mapping balik dari indeks ke User-ID
index_to_user_id = {idx: user_id for user_id, idx in user_id_to_index.items()}

"""Proses Encoding ISBN"""

# Mendapatkan ISBN unik dari data
unique_books = ratings_df['ISBN'].unique().tolist()

# Mapping ISBN ke index numerik
isbn_to_index = {isbn: idx for idx, isbn in enumerate(unique_books)}

# Mapping balik dari index ke ISBN
index_to_isbn = {idx: isbn for isbn, idx in isbn_to_index.items()}

# Tambahkan kolom hasil encoding ke dataframe
ratings_df['user_index'] = ratings_df['User-ID'].map(user_id_to_index)
ratings_df['book_index'] = ratings_df['ISBN'].map(isbn_to_index)

# Hitung total user dan total buku yang unik
total_users = len(user_id_to_index)
total_books = len(isbn_to_index)

print("Total users:", total_users)
print("Total books:", total_books)

# Mengubah rating ke float32
ratings_df['score'] = ratings_df['Book-Rating'].astype(np.float32)

# Mendapatkan rating minimum dan maksimum
min_score = ratings_df['score'].min()
max_score = ratings_df['score'].max()

print(f"Users: {total_users}, Books: {total_books}, Min Rating: {min_score}, Max Rating: {max_score}")

""" Sampling Dataset"""

# Ambil 10.000 sample secara acak untuk efisiensi training
# Check if the number of rows is less than the desired sample size
if ratings_df.shape[0] < 10000:
    # Use the entire DataFrame if it's smaller than 10000
    ratings_df = ratings_df
else:
    # Sample 10000 rows if the DataFrame is large enough
    ratings_df = ratings_df.sample(n=10000, random_state=42)

ratings_df.head()

"""Persiapan Data Model"""

# Membuat fitur (user dan buku) dan label (rating dinormalisasi)
features = ratings_df[['user_index', 'book_index']].to_numpy()
labels = ratings_df['score'].apply(lambda s: (s - min_score) / (max_score - min_score)).to_numpy()

# Split data menjadi 80% training dan 20% validasi
train_split = int(0.8 * len(features))
x_train, x_val = features[:train_split], features[train_split:]
y_train, y_val = labels[:train_split], labels[train_split:]

"""TF-IDF Author"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TF-IDF Vectorizer untuk teks author
vectorizer = TfidfVectorizer()
# Mengganti 'data' dengan 'dataset' yang sudah didefinisikan sebelumnya
vectorizer.fit(dataset['book_author'])

# Menampilkan fitur hasil ekstraksi
print("Fitur author:", vectorizer.get_feature_names_out())

"""Matriks TF-IDF"""

# Mengubah data teks author menjadi matriks TF-IDF
tfidf_result = vectorizer.transform(dataset['book_author'])

# Buat dataframe untuk menampilkan hasil TF-IDF
tfidf_df = pd.DataFrame(
    tfidf_result.todense(),
    columns=vectorizer.get_feature_names_out(),
    index=dataset['book_title'] # Also change the index here for consistency
)

# Tampilkan sebagian kecil hasil untuk contoh
tfidf_df.sample(10, axis=0).sample(22, axis=1)

"""Definisi Model Rekomendasi"""

from tensorflow.keras import Model
from tensorflow.keras import layers, regularizers
import tensorflow as tf
import numpy as np

# Definisikan arsitektur model rekomendasi
class BookRecommender(Model):
    def __init__(self, user_count, book_count, emb_dim):
        super().__init__()
        # Layer embedding untuk user dan buku
        self.user_emb = layers.Embedding(user_count, emb_dim, embeddings_initializer='he_normal',
                                         embeddings_regularizer=regularizers.l2(1e-6))
        self.user_bias = layers.Embedding(user_count, 1)
        self.book_emb = layers.Embedding(book_count, emb_dim, embeddings_initializer='he_normal',
                                         embeddings_regularizer=regularizers.l2(1e-6))
        self.book_bias = layers.Embedding(book_count, 1)

    def call(self, inputs):
        # Hitung dot product + bias
        u_vec = self.user_emb(inputs[:, 0])
        b_vec = self.book_emb(inputs[:, 1])
        u_bias = self.user_bias(inputs[:, 0])
        b_bias = self.book_bias(inputs[:, 1])
        dot = tf.reduce_sum(u_vec * b_vec, axis=1, keepdims=True)
        return tf.nn.sigmoid(dot + u_bias + b_bias)

# Inisialisasi dan compile model
model = BookRecommender(total_users, total_books, 50)
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Training model
training_history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=12,
    epochs=25,
    validation_data=(x_val, y_val)
)

# Menampilkan grafik RMSE untuk train dan validation
plt.plot(training_history.history['root_mean_squared_error'])
plt.plot(training_history.history['val_root_mean_squared_error'])
plt.title('Model Error Metrics')
plt.ylabel('RMSE')
plt.xlabel('Epoch')
plt.legend(['Training', 'Validation'], loc='upper left')
plt.show()

"""Prediksi Buku untuk User"""

# Ambil 1 sample user dari dataset
user_sample = ratings_df['User-ID'].sample(1).iloc[0]

# Cari buku yang sudah dibaca user
visited_books = ratings_df[ratings_df['User-ID'] == user_sample]

# Cari buku yang belum dibaca user
# Menggunakan 'books_final' yang berisi informasi buku
book_candidates = books_final[~books_final['id'].isin(visited_books['ISBN'])]
book_candidates = book_candidates[book_candidates['id'].isin(isbn_to_index)]

# Siapkan input prediksi untuk user tersebut
book_indices = [[isbn_to_index[isbn]] for isbn in book_candidates['id']]
user_idx = user_id_to_index[user_sample]
input_array = np.hstack((np.full((len(book_indices), 1), user_idx), book_indices))

"""Output Rekomendasi"""

# Prediksi skor rekomendasi
predicted_scores = model.predict(input_array).flatten()
top_indices = predicted_scores.argsort()[-10:][::-1]

# Ambil ISBN dari 10 prediksi terbaik
recommended_isbns = [index_to_isbn[book_indices[i][0]] for i in top_indices]

# Tampilkan hasil
print(f"Rekomendasi untuk user: {user_sample}")
print("===" * 10)

# Buku dengan rating tertinggi yang sudah dibaca user
top_user_books = visited_books.sort_values(by='score', ascending=False).head(5)['ISBN']
# Mengganti books_new dengan books_final yang sudah didefinisikan
for book in books_final[books_final['id'].isin(top_user_books)].itertuples():
    print(f"{book.book_title} oleh {book.book_author}")

print("---" * 10)
print("Top 10 Rekomendasi:")
# Mengganti books_new dengan books_final yang sudah didefinisikan
for book in books_final[books_final['id'].isin(recommended_isbns)].itertuples():
    print(f"{book.book_title} oleh {book.book_author}")